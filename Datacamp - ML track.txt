1.
from sklearn.linear_model import LogisticRegression

print(mod.predict(new_data))



2.
from sklearn.linear_model import LinearRegression
from sklearn import metrics

lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
y_pred = lin_reg.predict(X_test)

score = metrics.mean_squared_error(y_test, y_pred)
print(score)



3.
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
predictions = lin_reg.predict(X_test)
print("Mean squared error: %.2f" % mean_squared_error(y_test, predictions))


4.
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

model = DecisionTreeClassifier(max_depth=4, random_state=1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(accuracy_score(y_test, y_pred))


5.
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(random_state=1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
score = model.score(X_test, y_test)
print(score)



6.
predictions = customer_model.predict(X_new)

print(predictions)



7.
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(random_state=1)
model.fit(x, y)

print(model.score(x, y))



8.
Which of the following situations is best suited for the utilization of linear regression?
A retail company that wants to predict their total sales to understand the impact of online promotions.



9.
import numpy as np

result = np.mean(x)
print(result)



10.
What is principal component analysis (PCA) used for?
PCA summarizes the original dataset to one with fewer variables, called principal components, that are combinations of the original variables.



11.
from scipy.cluster.vq import kmeans, vq

clusters = kmeans(
	df[['x_scaled', 'y_scaled']],
	2
)

print(clusters)




12.
What is the optimal number of clusters for this data?
2


13.
What is the main difference between hierarchical clustering and k-means clustering?
K-means clustering requires a pre-specified number of clusters, while hierarchical clustering creates multiple clusters across all of the data.



14.
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

model = RandomForestClassifier(n_estimators=10, random_state=1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(accuracy_score(y_test, y_pred))




15.
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

model_params = {'n_estimators': [50, 150, 250]}

rf = RandomForestClassifier(random_state=42)

clf = GridSearchCV(rf, model_params, cv=5)

clf.fit(X_train, y_train)

print(clf.best_params_)



16.
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca.fit(scaled_samples)

pca_features = pca.transform(scaled_samples)
print(pca_features.shape)



17.
Which one of the following statements describes an unsupervised learning problem?
A machine learning problem where we seek to understand whether observations fit into distinct groups based on their similarities.


18.
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

reg = LinearRegression()
reg.fit(x, y)

print("Regression coefficients: {}".format(reg.coef_))
print("Regression intercept: {}".format(reg.intercept_))



19.
Overfitting means:
We have built a model that will only perform well on the training data.



20.
Which one of the following statements best describes k-means clustering and its utility?
K-means clustering groups data into relatively distinct groups by using a pre-determined number of clusters and iterating cluster assignments.



21.
Before we fit a model to our data we should consider centering and scaling the data so that:
A feature does not have more influence on the model because of larger or smaller values


22.
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(random_state=1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
score = model.score(X_test, y_test)
print(score)



23.
Which of the following situations is best suited for the utilization of linear regression?
A retail company that wants to predict their total sales to understand the impact of online promotions.



24.
Which of the following metrics would not be used when assessing the performance of a classification model?
Median absolute error.



25.
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

model_params = {'n_estimators': [50, 150, 250]}

rf = RandomForestClassifier(random_state=42)

clf = GridSearchCV(rf, model_params, cv=5)

clf.fit(X_train, y_train)

print(clf.best_params_)



26.
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.metrics import accuracy_score

# Create the model
clf = DecisionTreeClassifier(random_state=1)
# Fit the model
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print(accuracy_score(y_test, y_pred))




27.
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

model = DecisionTreeClassifier(max_depth=4, random_state=1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(accuracy_score(y_test, y_pred))




28.
import pandas as pd

encoded = pd.get_dummies(df, columns=['Animal'])
print(encoded)



29.
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score

dt = DecisionTreeClassifier(random_state=1)
bc = BaggingClassifier(base_estimator=dt, 
                       n_estimators=10,
                       random_state=1)

bc.fit(X_train, y_train)

y_pred = bc.predict(X_test)

print(accuracy_score(y_test, y_pred))



30.
How does a random forest improve upon a decision tree?
A random forest reduces variance by combining decision trees thatâ€¦

have been trained on different bootstrap samples of the training set.




31.
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

model = RandomForestClassifier(n_estimators=300, max_depth=1, random_state=1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(accuracy_score(y_test, y_pred))



32.
from sklearn.model_selection import train_test_split
from sklearn import metrics

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25, random_state=42)
clf.fit(X_train,y_train)

y_pred=clf.predict(X_test)

score = metrics.accuracy_score(y_test, y_pred)
print(score)



33.
from sklearn.linear_model import LinearRegression

model = LinearRegression(fit_intercept=True)

model.fit(x, y)

print("Regression coefficients: {}".format(model.coef_))
print("Regression intercept: {}".format(model.intercept_))



34.
import pandas as pd

encoded = pd.get_dummies(df, columns=['Animal'])
print(encoded)




35.
from sklearn.ensemble import GradientBoostingClassifier

model = GradientBoostingClassifier(n_estimators=300, learning_rate=0.01, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(model.score(X_test, y_test))



36.
from sklearn import model_selection

X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.8, random_state=42)
print("X_train shape: ", X_train.shape)
print("X_test shape: ", X_test.shape)
print("y_train shape: ",y_train.shape)
print("y_test shape: ",y_test.shape)


37.
from sklearn.linear_model import LogisticRegression

print(mod.predict(new_data))



38.
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

new_data = np.array([[8, 9]]).reshape(-1,1)

predictions = model.predict(new_data)
print(predictions)



39.
from sklearn import metrics

mse1=metrics.mean_squared_error(actual, pred_1)
mse2=metrics.mean_squared_error(actual, pred_2)

print("MSE for model 1 is", str(mse1.round(2)), 
      "and for model 2 is", str(mse2.round(2)))





40.
will the customer get approved for a new credit card?
No, because the customer has 5 existing credit cards and medium income.



41.
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Lasso

lasso_model = Lasso(alpha=0.01)

lasso_model.fit(X_train, y_train)

lasso_predictions = lasso_model.predict(X_test)
print("RMSE: ", np.sqrt(mean_squared_error(y_test,lasso_predictions)))



42.
Which of the following is correct?
rf_3 has high variance because it is performing well on training and not so well on testing.



43.
from sklearn.preprocessing import PowerTransformer

log = PowerTransformer(method='yeo-johnson')
df['log_x'] = log.fit_transform(df[['x']])
print(df['log_x'].head())



44.
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

clf=LogisticRegression(solver='newton-cg', random_state=42)
clf.fit(X_train,y_train)

y_pred=clf.predict(X_test)

size = metrics.roc_auc_score(y_test, y_pred)
print(size)



45.











































